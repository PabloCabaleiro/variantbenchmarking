# nf-core/variantbenchmarking: Output

## Introduction

This document describes the output produced by the pipeline. Most of the plots are taken from the MultiQC report, which summarizes results at the end of the pipeline.

The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.

<!-- TODO nf-core: Write this documentation describing your workflow's output -->

## Pipeline overview

The pipeline is built using [Nextflow](https://www.nextflow.io/) and processes data using the following steps:

- [Preprocesses ](#preprocesses)
- [Liftover of truth sets](#liftover)
- [Input vcf statistics](#stats)
- [Benchmarking](#bench)
  - [Truvari](#truvari_bench)
  - [SVanalyzer](#svanalyzer_bench)
  - [Wittyer](#wittyer_bench)
  - [RTG-tools](#rtgtools_bench)
  - [Happy](#happy_bench)
  - [Sompy](#sompy_bench)
- [Summary statistics](#summary)
  - [Comparison of benchmarking results](#comparisons)
  - [Merged summary benchmark statistics](#tables)
  - [Plots](#plots)
  - [datavzrd HTML reports](#html)
- [MultiQC](#multiqc) - Aggregate report describing results and QC from the whole pipeline
- [Pipeline information](#pipeline-information) - Report metrics generated during the workflow execution

### Preprocesses

<details markdown="1">
<summary>Output files</summary>

- `preprocesses/`
  - `*vcf.gz`: The standardized and normalized VCF files

</details>

Outputs from standardization, normalization and filtration processes saved. When any of `--sv_standardization`, `--preprocesses` or filtration applied to the input set of variants, the processed outputs will be saved into this directory.

For test VCFs structuring as follows:

*id*/*preprocess*/

For truth VCFs structuring as follows:

*sample*/*preprocess*/

### Liftover of truth sets

<details markdown="1">
<summary>Output files</summary>

- `liftover/`
  -

</details>

If liftover applied ...

### Input VCF statistics

<details markdown="1">
<summary>Output files</summary>

- `stats/`
  - `bcftools/`
    - '*.bcftools_stats.txt'
  - `survivor/`
    - '*.stats'

</details>

bcftools stats applied into all variant types while survivor stats is only available for structural variants.


### Benchmarking

<details markdown="1">
<summary>Output files</summary>

- `truvari_bench/`
  - `*.fn.vcf.gz`
  - `*.fn.vcf.gz.tbi`
  - `*.fp.vcf.gz`
  - `*.fp.vcf.gz.tbi`
  - `*.tp-comp.vcf.gz`
  - `*.tp-comp.vcf.gz.tbi`
  - `*.tp-base.vcf.gz`
  - `*.tp-base.vcf.gz.tbi`
  - `*.summary.json`
- `svanalyzer_bench/`
  - `*.distances`
  - `*.falsenegatives.vcf.gz`
  - `*.falsepositives.vcf.gz`
  - `*.log`
  - `*.report`
- `wittyer_bench/`
  - `*.vcf.gz`
  - `*.vcf.gz.tbi`
  - `*.json`
- `rtgtools_bench/`
  - `*.vcf.gz`
  - `*.vcf.gz.tbi`
  - `*.fn.vcf.gz`
  - `*.fn.vcf.gz.tbi`
  - `*.fp.vcf.gz`
  - `*.fp.vcf.gz.tbi`
  - `*.tp.vcf.gz`
  - `*.tp.vcf.gz.tbi`
  - `*.tp-baseline.vcf.gz`
  - `*.tp-baseline.vcf.gz.tbi`
  - `*.non_snp_roc.tsv.gz`
  - `*.phasing.txt`
  - `*.snp_roc.tsv.gz`
  - `*.summary.txt`
  - `*.weighted_roc.tsv.gz`
- `happy_bench/`
  - `*.extended.csv`
  - `*.metrics.json.gz`
  - `*.roc.all.csv.gz`
  - `*.roc.Locations.INDEL.csv.gz`
  - `*roc.Locations.INDEL.PASS.csv.gz`
  - `*roc.Locations.SNP.csv.gz`
  - `*roc.Locations.SNP.PASS.csv.gz`
  - `*.runinfo.json`
  - `*.summary.csv`
  - `*.vcf.gz`
  - `*.vcf.gz.tbi`
- `sompy_bench/`
  -

</details>

Benchmark results are created separately for each test vcf:

*id*/*_bench_*/


### Summary statistics

<details markdown="1">
<summary>Output files</summary>

- `comparisons/`
  -
- `plots/`
  -
- `tables/`
  -
- `html/`
  -

</details>
......

### MultiQC

<details markdown="1">
<summary>Output files</summary>

- `multiqc/`
  - `multiqc_report.html`: a standalone HTML file that can be viewed in your web browser.
  - `multiqc_data/`: directory containing parsed statistics from the different tools used in the pipeline.
  - `multiqc_plots/`: directory containing static images from the report in various formats.

</details>

[MultiQC](http://multiqc.info) is a visualization tool that generates a single HTML report summarising all samples in your project. Most of the pipeline QC results are visualised in the report and further statistics are available in the report data directory.

Results generated by MultiQC collate pipeline QC from supported tools e.g. FastQC. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see <http://multiqc.info>.

### Pipeline information

<details markdown="1">
<summary>Output files</summary>

- `pipeline_info/`
  - Reports generated by Nextflow: `execution_report.html`, `execution_timeline.html`, `execution_trace.txt` and `pipeline_dag.dot`/`pipeline_dag.svg`.
  - Reports generated by the pipeline: `pipeline_report.html`, `pipeline_report.txt` and `software_versions.yml`. The `pipeline_report*` files will only be present if the `--email` / `--email_on_fail` parameter's are used when running the pipeline.
  - Reformatted samplesheet files used as input to the pipeline: `samplesheet.valid.csv`.
  - Parameters used by the pipeline run: `params.json`.

</details>

[Nextflow](https://www.nextflow.io/docs/latest/tracing.html) provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.
